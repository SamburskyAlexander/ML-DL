{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Обработка текстов\n",
    "\n",
    "Предполагается зафиксированными следующие гиперпараметры модели:\n",
    "- размер батча: 64;\n",
    "- длина обрезанного токенизированного предложения: 200;\n",
    "- размерность результата эмбеддинга для 1 предложения: 32;\n",
    "- размерность hidden в рекуррентной нейронной сети;\n",
    "- выход модели (после линейного слоя): 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_dim, hidden_dim, output_size, vocab,\n",
    "        rec_layer=torch.nn.LSTM, dropout=None, **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_size = output_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        self.word_embeddings = torch.nn.Embedding(num_embeddings=len(self.vocab.stoi), embedding_dim=self.embedding_dim, padding_idx=0)\n",
    "        if dropout is not None:\n",
    "            self.rnn = rec_layer(self.embedding_dim, self.hidden_dim, dropout=self.dropout, **kwargs)\n",
    "        else:\n",
    "            self.rnn = rec_layer(self.embedding_dim, self.hidden_dim, **kwargs)\n",
    "        self.Linear = torch.nn.Linear(self.hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, tokens, tokens_lens):\n",
    "        \"\"\"\n",
    "        :param torch.tensor(dtype=torch.long) tokens: Batch of texts represented with tokens.\n",
    "        :param torch.tensor(dtype=torch.long) tokens_lens: Number of non-padding tokens for each object in batch.\n",
    "        :return torch.tensor(dtype=torch.long): Vector representation for each sequence in batch\n",
    "        \"\"\"\n",
    "                                                                                ### tokens.shape = (200, 64) Число слов в токенизированном предложении на число предложений в батче (под предложением подразумевается её токенизированная версия)\n",
    "                                                                                ### tokens_lens.shape = (64) Число предложений в батче\n",
    "        out = self.word_embeddings(tokens)                                      ### out.shape = (200, 64, 32), 32 - это размерность вектора эмбеддинга, которым мы \"кодируем\" каждый токен\n",
    "        \n",
    "        out = self.rnn(out)[0]                                                  ### self.rnn(out) = (output, (h, c)), output.shape = (200, 64, 128), h.shape = (1, 64, 128), c.shape = (1, 64, 128)\n",
    "                                                                                ### Откуда взялось 128: мы подаём эмбеддинговые представления в ячейку LSTM, в которой перед агрегацией состояния ячейки с входными данными происходит преобразование линейного слоя, и эмбеддинговые представления токенов из 32-размерных становятся 128-размерными\n",
    "\n",
    "        out = out[tokens_lens - 1, np.arange(out.shape[1]), :]                  ### out.shape = (64, 128)\n",
    "        res = self.Linear(out)                                                  ### res.shape = (64, 10)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, model, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for idx, data in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # 1. Take data from batch\n",
    "        tokens, tokens_lens, ratings = data['tokens'].to(device), data['tokens_lens'].to(device), data['ratings'].to(device)\n",
    "        # 2. Perform forward pass\n",
    "        model_out = model(tokens, tokens_lens).to(device)\n",
    "        # 3. Evaluate loss\n",
    "        loss = loss_fn(model_out, ratings)\n",
    "        loss.backward()\n",
    "        # 4. Make optimizer step\n",
    "        optimizer.step()\n",
    "    \n",
    "def evaluate(dataloader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dataloader):\n",
    "            # 1. Take data from batch\n",
    "            tokens, tokens_lens, ratings = data['tokens'].to(device), data['tokens_lens'].to(device), data['ratings'].to(device)\n",
    "            # 2. Perform forward pass\n",
    "            model_out = model(tokens, tokens_lens).to(device)\n",
    "            # 3. Evaluate loss\n",
    "            loss = loss_fn(model_out, ratings)\n",
    "            total_loss += float(loss.detach())\n",
    "            # 4. Evaluate accuracy\n",
    "            total_accuracy += torch.sum(1*(torch.argmax(model_out, dim=1) == ratings))\n",
    "        \n",
    "    return total_loss / len(dataloader.dataset), total_accuracy / len(dataloader.dataset)\n",
    "\n",
    "def train(train_loader, test_loader, model, loss_fn, optimizer, device, num_epochs):\n",
    "    test_losses = []\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    train_accuracies = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch(train_loader, model, loss_fn, optimizer, device)\n",
    "        \n",
    "        train_loss, train_acc = evaluate(train_loader, model, loss_fn, device)\n",
    "        train_accuracies.append(train_acc)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        test_loss, test_acc = evaluate(test_loader, model, loss_fn, device)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        \n",
    "        print(\n",
    "            'Epoch: {0:d}/{1:d}. Loss (Train/Test): {2:.3f}/{3:.3f}. Accuracy (Train/Test): {4:.3f}/{5:.3f}'.format(\n",
    "                epoch + 1, num_epochs, train_losses[-1], test_losses[-1], train_accuracies[-1], test_accuracies[-1]\n",
    "            )\n",
    "        )\n",
    "    return train_losses, train_accuracies, test_losses, test_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация дропаута по статье Гала и Гарамани. Variational Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_h0_c0(num_objects, hidden_size, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    return h0 and c0, use some_existing_tensor.new_zeros() to gen them\n",
    "    h0 shape: num_objects x hidden_size\n",
    "    c0 shape: num_objects x hidden_size\n",
    "    \"\"\"\n",
    "    return (some_existing_tensor.new_zeros(size=(num_objects, hidden_size)), \n",
    "            some_existing_tensor.new_zeros(size=(num_objects, hidden_size)))\n",
    "\n",
    "def gen_dropout_mask(input_size, hidden_size, is_training, p, some_existing_tensor):\n",
    "    \"\"\"\n",
    "    is_training: if True, gen masks from Bernoulli\n",
    "                 if False, gen masks consisting of (1-p)\n",
    "    \n",
    "    return dropout masks of size input_size, hidden_size if p is not None\n",
    "    return one masks if p is None\n",
    "    \"\"\"\n",
    "    if p is None:\n",
    "        return (some_existing_tensor.new_ones(hidden_size),\n",
    "                some_existing_tensor.new_ones(input_size))\n",
    "    else:\n",
    "        if is_training:\n",
    "            return (torch.bernoulli(some_existing_tensor.new_ones(hidden_size) * (1 - p)),\n",
    "                    torch.bernoulli(some_existing_tensor.new_ones(input_size) * (1 - p)))\n",
    "        else:\n",
    "            return (some_existing_tensor.new_ones(hidden_size) * (1 - p),\n",
    "                    some_existing_tensor.new_ones(input_size) * (1 - p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn_cell = torch.nn.LSTMCell(self.input_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "                                                                                ### x.shape = (200, 64, 32)\n",
    "        h_0, c_0 = init_h0_c0(num_objects=x.shape[1], \n",
    "                              hidden_size=self.hidden_size, \n",
    "                              some_existing_tensor=x)                           ### h_0.shape = c_0.shape = (64, 128)\n",
    "        \n",
    "        m_h, m_x = gen_dropout_mask(input_size=self.input_size, \n",
    "                                    hidden_size=self.hidden_size, \n",
    "                                    is_training=self.training, \n",
    "                                    p=self.dropout, \n",
    "                                    some_existing_tensor=x)                     ### m_h.shape = (64, 128), m_x.shape = (64, 32)\n",
    "    \n",
    "        cell_output = []\n",
    "        h, c = h_0, c_0\n",
    "        for t in range(x.shape[0]):\n",
    "            h, c = self.rnn_cell(x[t] * m_x, (h * m_h, c))                      ### h.shape = c.shape = (64, 128)\n",
    "            cell_output.append(h)\n",
    "\n",
    "        output = torch.stack(cell_output, dim=0)                                ### output.shape = (200, 64, 128)\n",
    "        return output, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эффективная реализация дропаута по статье Гала и Гарамани."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastRNNLayer(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=None, layers_dropout=0.0, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.layers_dropout = layers_dropout\n",
    "        self.module = torch.nn.LSTM(input_size, hidden_size, dropout=layers_dropout, num_layers=num_layers)\n",
    "\n",
    "        self.layer_names = []\n",
    "        for layer_n in range(self.num_layers):\n",
    "            self.layer_names += [f'weight_hh_l{layer_n}', f'weight_ih_l{layer_n}']\n",
    "\n",
    "        for layer in self.layer_names:\n",
    "            # Get torch.nn.Parameter with weights from torch.nn.LSTM instance\n",
    "            w = getattr(self.module, layer)\n",
    "\n",
    "            # Remove it from model\n",
    "            delattr(self.module, layer)\n",
    "\n",
    "            # And create new torch.nn.Parameter with the same data but different name\n",
    "            self.register_parameter(f'{layer}_raw', torch.nn.Parameter(w.data))\n",
    "\n",
    "    def _setweights(self, x):\n",
    "        \"\"\"\n",
    "            Apply dropout to the raw weights.\n",
    "        \"\"\"\n",
    "        for layer in self.layer_names:\n",
    "            # Get torch.nn.Parameter with weights\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "\n",
    "            h_m, x_m = gen_dropout_mask(input_size=self.input_size, \n",
    "                                        hidden_size=self.hidden_size, \n",
    "                                        is_training=self.training,\n",
    "                                        p=self.dropout, some_existing_tensor=x)\n",
    "\n",
    "            # Apply dropout mask\n",
    "            if raw_w.shape[1] == h_m.shape[0]:\n",
    "                masked_raw_w = raw_w * h_m\n",
    "            else:\n",
    "                masked_raw_w = raw_w * x_m\n",
    "\n",
    "            # Set modified weights in its place\n",
    "            setattr(self.module, layer, masked_raw_w)\n",
    "\n",
    "    def forward(self, x, h_c=None):\n",
    "        \"\"\"\n",
    "        :param x: tensor containing the features of the input sequence.\n",
    "        :param Optional[Tuple[torch.tensor, torch.tensor]] h_c: initial hidden state and initial cell state\n",
    "        \"\"\"\n",
    "        with warnings.catch_warnings():\n",
    "            # To avoid the warning that comes because the weights aren't flattened.\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            # Set new weights of self.module and call its forward\n",
    "            # Pass h_c with x if it is not None. Otherwise pass only x\n",
    "            # YOUR CODE HERE\n",
    "            self._setweights(x)\n",
    "            if not h_c is None:\n",
    "                return self.module.forward(x, h_c)\n",
    "            else:\n",
    "                return self.module.forward(x)\n",
    "            \n",
    "    def reset(self):\n",
    "        if hasattr(self.module, 'reset'):\n",
    "            self.module.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация дропаута по статье Семениуты и др."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandmadeLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_weights = torch.nn.Linear(input_size, 4 * hidden_size)\n",
    "        self.hidden_weights = torch.nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        \n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        \"\"\"\n",
    "        Initialization as in Pytorch. \n",
    "        Do not forget to call this method!\n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "                                                                                ### x.shape = (200, 64, 32)\n",
    "        h_0, c_0 = init_h0_c0(num_objects=x.shape[1], \n",
    "                              hidden_size=self.hidden_size, \n",
    "                              some_existing_tensor=x)                           ### h_0.shape = c_0.shape = (64, 128)\n",
    "\n",
    "        m_h, m_x = gen_dropout_mask(input_size=self.input_size, \n",
    "                                    hidden_size=self.hidden_size, \n",
    "                                    is_training=self.training, \n",
    "                                    p=self.dropout, \n",
    "                                    some_existing_tensor=x)                     ### m_h.shape = (64, 128), m_x.shape = (64, 32)\n",
    "        \n",
    "        # Implement recurrent logic to mimic torch.nn.LSTM\n",
    "        cell_output = []\n",
    "        h, c = h_0, c_0\n",
    "        for t in range(x.shape[0]):\n",
    "            iofg = self.hidden_weights(h) + self.input_weights(x[t] * m_x)      ### iofg.shape = (64, 4*128)\n",
    "            iof = torch.sigmoid(iofg[:, :3*self.hidden_size])                   ### iof.shape = (64, 3*128)\n",
    "            i = iof[:, :self.hidden_size]                                       ### i.shape = (64, 128)\n",
    "            o = iof[:, self.hidden_size:2*self.hidden_size]                     ### o.shape = (64, 128)\n",
    "            f = iof[:, 2*self.hidden_size:3*self.hidden_size]                   ### f.shape = (64, 128)\n",
    "            g = torch.tanh(iofg[:, 3*self.hidden_size:])                        ### g.shape = (64, 128)\n",
    "            c = f * c + i * g * m_h                                             ### c.shape = (64, 128)\n",
    "            h = o * torch.tanh(c)                                               ### h.shape = (64, 128)\n",
    "            \n",
    "            cell_output.append(h)               \n",
    "        output = torch.stack(cell_output, dim=0)                                ### output.shape = (200, 64, 128)\n",
    "        return output, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Реализация Zoneout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zoneout(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.input_weights = torch.nn.Linear(input_size, 4 * hidden_size)\n",
    "        self.hidden_weights = torch.nn.Linear(hidden_size, 4 * hidden_size)\n",
    "        \n",
    "        self.reset_params()\n",
    "\n",
    "    def reset_params(self):\n",
    "        \"\"\"\n",
    "        Initialization as in Pytorch. \n",
    "        Do not forget to call this method!\n",
    "        https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#LSTM\n",
    "        \"\"\"\n",
    "        stdv = 1.0 / np.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "                                                                                ### x.shape = (200, 64, 32)\n",
    "        h_0, c_0 = init_h0_c0(num_objects=x.shape[1], \n",
    "                              hidden_size=self.hidden_size, \n",
    "                              some_existing_tensor=x)                           ### h_0.shape = c_0.shape = (64, 128)\n",
    "\n",
    "        m_x = gen_dropout_mask(input_size=self.input_size, \n",
    "                                    hidden_size=self.hidden_size, \n",
    "                                    is_training=self.training, \n",
    "                                    p=self.dropout, \n",
    "                                    some_existing_tensor=x)[1]                  ### m_h.shape = (64, 128), m_x.shape = (64, 32)\n",
    "        \n",
    "        # Implement recurrent logic to mimic torch.nn.LSTM\n",
    "        cell_output = []\n",
    "        h, c = h_0, c_0\n",
    "        for t in range(x.shape[0]):\n",
    "\n",
    "            m_h = gen_dropout_mask(input_size=self.input_size, \n",
    "                                    hidden_size=self.hidden_size, \n",
    "                                    is_training=self.training, \n",
    "                                    p=self.dropout, \n",
    "                                    some_existing_tensor=x)[0]                  ### m_h.shape = (64, 128), m_x.shape = (64, 32)\n",
    "            iofg = self.hidden_weights(h) + self.input_weights(x[t] * m_x)      ### iofg.shape = (64, 4*128)\n",
    "            iof = torch.sigmoid(iofg[:, :3*self.hidden_size])                   ### iof.shape = (64, 3*128)\n",
    "            i = iof[:, :self.hidden_size]                                       ### i.shape = (64, 128)\n",
    "            o = iof[:, self.hidden_size:2*self.hidden_size]                     ### o.shape = (64, 128)\n",
    "            f = iof[:, 2*self.hidden_size:3*self.hidden_size]                   ### f.shape = (64, 128)\n",
    "            g = torch.tanh(iofg[:, 3*self.hidden_size:])                        ### g.shape = (64, 128)\n",
    "            c = f * c + i * g                                                   ### c.shape = (64, 128)\n",
    "            h = m_h * (o * torch.tanh(c)) + (1 - m_h) * h                       ### h.shape = (64, 128)\n",
    "            \n",
    "            cell_output.append(h)               \n",
    "        output = torch.stack(cell_output, dim=0)                                ### output.shape = (200, 64, 128)\n",
    "        return output, (h, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* # Сегментация изображений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модуль аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flipping:\n",
    "    def __init__(self, p=1):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask=None):\n",
    "        p_sample = np.random.rand()\n",
    "        if p_sample < self.p:\n",
    "            flip_matrix = np.fromfunction(lambda i, j: i + j == img.shape[2] - 1, (img.shape[2], img.shape[2])).astype(int)\n",
    "            return torch.from_numpy(np.dot(img, flip_matrix)), torch.from_numpy(np.dot(mask, flip_matrix))\n",
    "        return img.double(), mask.double()\n",
    "    \n",
    "    \n",
    "class Cutting:\n",
    "    def __init__(self, p=1):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask=None, size=None):\n",
    "        p_sample = np.random.rand()\n",
    "        if p_sample < self.p:\n",
    "            if size is None:\n",
    "                a = np.random.randint(0, img.shape[1] // 4)\n",
    "                b = np.random.randint(img.shape[1] * 3 // 4, img.shape[1])\n",
    "                c = np.random.randint(0, img.shape[2] // 4)\n",
    "                d = np.random.randint(img.shape[2] * 3 // 4, img.shape[2])\n",
    "            else:\n",
    "                a, b, c, d = size\n",
    "            cut_matrix = np.ones((img.shape[1], img.shape[2]))\n",
    "            cut_matrix *= np.fromfunction(lambda i, j: (i - a)*(i - b) <= 0, (img.shape[1], img.shape[2])).astype(int)\n",
    "            cut_matrix *= np.fromfunction(lambda i, j: (j - c)*(j - d) <= 0, (img.shape[1], img.shape[2])).astype(int)\n",
    "            return img * cut_matrix, mask * cut_matrix\n",
    "        return img.double(), mask.double()\n",
    "    \n",
    "    \n",
    "class Brighting:\n",
    "    def __init__(self, p=1):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask=None, delta=None):\n",
    "        p_sample = np.random.rand()\n",
    "        if p_sample < self.p:\n",
    "            if delta == None:\n",
    "                delta = np.random.randn()\n",
    "            img += delta\n",
    "            img[img < 0] = 0\n",
    "            img[img > 1] = 1    \n",
    "            return img, mask\n",
    "        return img.double(), mask.double()\n",
    "    \n",
    "    \n",
    "class Screening:\n",
    "    def __init__(self, p=1):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img, mask=None, screen=None):\n",
    "        p_sample = np.random.rand()\n",
    "        if p_sample < self.p:\n",
    "            screen = screen.type(img.dtype)\n",
    "            the_mask = [mask[0] + mask[1] + mask[2] == 0]\n",
    "            img[0][the_mask] = screen[0][the_mask]\n",
    "            img[1][the_mask] = screen[1][the_mask]\n",
    "            img[2][the_mask] = screen[2][the_mask]\n",
    "            return img, mask\n",
    "        return img.double(), mask.double()\n",
    "    \n",
    "    \n",
    "def sequential_transforms(img, mask, propabilities=(1, 1, 1, 1), params=(None, None, None)):\n",
    "    img = torchvision.transforms.ToTensor()(img)\n",
    "    img = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)\n",
    "    mask = torchvision.transforms.ToTensor()(mask)\n",
    "    img, mask = Flipping(propabilities[0])(img, mask)\n",
    "    if params[2] is None:\n",
    "        img0 = copy.deepcopy(img)\n",
    "    else:\n",
    "        img0 = params[2]\n",
    "    img, mask = Brighting(propabilities[1])(img, mask, params[0])\n",
    "    img, mask = Cutting(propabilities[2])(img, mask, params[1])\n",
    "    img, mask = Screening(propabilities[3])(img, mask, screen=img0)\n",
    "    return img, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, eps=1e-7, reduction=None, with_logits=True):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        eps : float\n",
    "            eps in denominator\n",
    "        reduction : Optional[str] (None, 'mean' or 'sum')\n",
    "            specifies the reduction to apply to the output:\n",
    "            \n",
    "            None: no reduction will be applied\n",
    "            'mean': the sum of the output will be divided by the number of batches in the output\n",
    "            'sum':  the output will be summed. \n",
    "        with_logits : bool\n",
    "            If True, use additional sigmoid for inputs\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "        self.with_logits = with_logits\n",
    "        \n",
    "    def forward(self, logits, true_labels):\n",
    "        true_labels = true_labels.float()\n",
    "        \n",
    "        if self.with_logits:\n",
    "            logits = torch.sigmoid(logits)\n",
    "        \n",
    "        # your code here\n",
    "        losses = 1 - 2 * torch.sum(logits * true_labels, axis=(1, 2)) / torch.sum(logits + true_labels + self.eps, axis=(1, 2))\n",
    "        \n",
    "        if self.reduction == 'sum':\n",
    "            loss_value = torch.sum(losses)\n",
    "        elif self.reduction == 'mean':\n",
    "            loss_value = torch.mean(losses)\n",
    "        elif self.reduction is None:\n",
    "            loss_value = losses\n",
    "        return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combination_Loss(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.first_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        self.second_criterion = DiceLoss(reduction='mean')\n",
    "        \n",
    "    def forward(self, net_out, target):\n",
    "        return self.alpha * self.first_criterion(net_out, target) + (1 - self.alpha) * self.second_criterion(net_out, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG13Encoder(torch.nn.Module):\n",
    "    def __init__(self, num_blocks, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.num_blocks = num_blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        # Obtaining pretrained VGG model from torchvision.models and\n",
    "        # copying all layers except for max pooling.\n",
    "        feature_extractor = vgg13(pretrained=pretrained).features\n",
    "        for i in range(self.num_blocks):\n",
    "            self.blocks.append(\n",
    "                torch.nn.Sequential(*[feature_extractor[j]\n",
    "                                      for j in range(i * 5, i * 5 + 4)]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = []\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.blocks[i](x)\n",
    "            activations.append(x)\n",
    "            if i != self.num_blocks - 1:\n",
    "                x = torch.functional.F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(torch.nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upconv = torch.nn.Conv2d(\n",
    "            in_channels=out_channels * 2, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=out_channels * 2, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=out_channels, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, down, left):\n",
    "        x = torch.nn.functional.interpolate(down, scale_factor=2)\n",
    "        x = self.upconv(x)\n",
    "        x = self.relu(self.conv1(torch.cat([left, x], 1)))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.add_module(f'block{num_blocks - i}', DecoderBlock(num_filters * 2**i))\n",
    "\n",
    "    def forward(self, acts):\n",
    "        up = acts[-1]\n",
    "        for i, left in enumerate(acts[-2::-1]):\n",
    "            up = self.__getattr__(f'block{i + 1}')(up, left)\n",
    "        return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=64, num_blocks=4):\n",
    "        super().__init__()\n",
    "        self.encoder = VGG13Encoder(num_blocks=num_blocks)\n",
    "        self.decoder = Decoder(num_filters=64, num_blocks=num_blocks - 1)\n",
    "        self.final = torch.nn.Conv2d(\n",
    "            in_channels=num_filters, out_channels=num_classes, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = self.encoder(x)\n",
    "        x = self.decoder(acts)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(outputs, labels, inline=False):\n",
    "    outputs = torch.nn.Sigmoid()(outputs)\n",
    "    outputs[outputs < 0.5] = 0\n",
    "    outputs[outputs >= 0.5] = 1\n",
    "    intersection = (outputs * labels).float().sum((1, 2))\n",
    "    union = (outputs + labels - outputs * labels).float().sum((1, 2))\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    if inline:\n",
    "        print('    acc on batch:', float(iou.mean()))\n",
    "    return iou.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_net(net, testloader, criterion, val_criterion=iou, device='cpu'):\n",
    "    net = net.eval()\n",
    "\n",
    "    loss = 0.\n",
    "    correct = 0.\n",
    "    total1 = 0.\n",
    "    total2 = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            labels = labels[:, 0, :, :]\n",
    "            images = images.to(device)\n",
    "            \n",
    "            outputs = net(images.float()).to('cpu').squeeze(1)\n",
    "            total1 += labels.size(0)\n",
    "            total2 += 1\n",
    "            loss += float(criterion(outputs, labels).detach())\n",
    "            correct += float(val_criterion(outputs, labels).detach())\n",
    "    \n",
    "    mean_loss = loss / total1\n",
    "    metric = correct / total2\n",
    "    \n",
    "    return mean_loss, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    times = {}\n",
    "    losses = {}\n",
    "    accs = {}\n",
    "    acc_test = []\n",
    "    acc_train = []\n",
    "    loss_test = []\n",
    "    loss_train = []\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch:', epoch)\n",
    "        times[epoch] = []\n",
    "        losses[epoch] = []\n",
    "        accs[epoch] = []\n",
    "        model = model.to(device)\n",
    "        model = model.train()\n",
    "        k = 0\n",
    "        buf_time = time.time() \n",
    "        for data, target in train_data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            net_out = model(data.float())\n",
    "            target = target[:, 0, :, :]\n",
    "\n",
    "            net_out = net_out.squeeze(1)\n",
    "          \n",
    "            loss = criterion(net_out, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print('  loss on batch ', k, ':', loss.item())\n",
    "            k += 1\n",
    "            end_time = time.time()\n",
    "            times[epoch] += [end_time - buf_time]\n",
    "            \n",
    "            losses[epoch] += [float(loss.item())]\n",
    "            accs[epoch] += [float(val_criterion(net_out, target, True))] ##\n",
    "            del loss \n",
    "        scheduler.step()\n",
    "\n",
    "        info_test = evaluate_net(model, test_data_loader, criterion, iou, 'cuda:0')\n",
    "        info_train = evaluate_net(model, train_data_loader, criterion, iou, 'cuda:0')\n",
    "        print('epoch result:')\n",
    "        print('...train:')\n",
    "        print('......accy:', info_train[1])\n",
    "        print('......loss:', info_train[0])\n",
    "        print('...test:')\n",
    "        print('......accy:', info_test[1])\n",
    "        print('......loss:', info_test[0])\n",
    "        acc_test += [info_test[1]]\n",
    "        acc_train += [info_train[1]]\n",
    "        loss_test += [info_test[0]]\n",
    "        loss_train += [info_train[0]]\n",
    "    torch.save(model, 'path_to_model.pth')\n",
    "    return accs, losses, times, acc_test, acc_train, loss_test, loss_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LinkNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock2(torch.nn.Module):\n",
    "    def __init__(self, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upconv = torch.nn.Conv2d(\n",
    "            in_channels=out_channels * 2, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=out_channels, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=out_channels, out_channels=out_channels,\n",
    "            kernel_size=3, padding=1, dilation=1\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, down, left):\n",
    "        x = torch.nn.functional.interpolate(down, scale_factor=2)\n",
    "        x = self.upconv(x)\n",
    "        x = self.relu(self.conv1(left + x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder2(torch.nn.Module):\n",
    "    def __init__(self, num_filters, num_blocks):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            self.add_module(f'block{num_blocks - i}', DecoderBlock2(num_filters * 2**i))\n",
    "\n",
    "    def forward(self, acts):\n",
    "        up = acts[-1]\n",
    "        for i, left in enumerate(acts[-2::-1]):\n",
    "            up = self.__getattr__(f'block{i + 1}')(up, left)\n",
    "        return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkNet(torch.nn.Module):\n",
    "    def __init__(self, num_classes=1, num_filters=64, num_blocks=4):\n",
    "        super().__init__()\n",
    "        self.encoder = VGG13Encoder(num_blocks=num_blocks)\n",
    "        self.decoder = Decoder2(num_filters=64, num_blocks=num_blocks - 1)\n",
    "        self.final = torch.nn.Conv2d(\n",
    "            in_channels=num_filters, out_channels=num_classes, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        acts = self.encoder(x)\n",
    "        x = self.decoder(acts)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
