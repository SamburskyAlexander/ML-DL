{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN + оценка по кросс-валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#============================================= ЗАДАНИЕ ЕВКЛИДОВОЙ И КОСИНУСНОЙ МЕТРИК ДЛЯ ОБЪЕКТОВ =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "def euclidean_distance(X, Y):\n",
    "    \"\"\"\n",
    "        ================================================\n",
    "        returns euclidean distance between X & Y objects\n",
    "        \n",
    "        ================================================\n",
    "        param .... type ............... meaning ........\n",
    "        ================================================\n",
    "        X ........ ndarray (float) .... data ...........\n",
    "        Y ........ ndarray (float) .... data ...........\n",
    "        ================================================\n",
    "    \"\"\"\n",
    "    X_part = (X * X).sum(axis = 1)[np.newaxis].T\n",
    "    Y_part = (Y * Y).sum(axis = 1)[np.newaxis]\n",
    "    XY_part = np.dot(X, Y.T)\n",
    "    return (-2 * XY_part + X_part + Y_part) ** 0.5\n",
    "\n",
    "def cosine_distance(X, Y):\n",
    "    \"\"\"\n",
    "        =============================================\n",
    "        returns cosine distance between X & Y objects\n",
    "        \n",
    "        =============================================\n",
    "        param .... type ............... meaning .....\n",
    "        =============================================\n",
    "        X ........ ndarray (float) .... data ........\n",
    "        Y ........ ndarray (float) .... data ........\n",
    "        =============================================\n",
    "    \"\"\"\n",
    "    X_part = ((X * X).sum(axis = 1) ** 0.5)[np.newaxis].T \n",
    "    Y_part = ((Y * Y).sum(axis = 1) ** 0.5)[np.newaxis]\n",
    "    XY_part = np.dot(X, Y.T)\n",
    "    return 1 - (XY_part / X_part / Y_part)\n",
    "\n",
    "\n",
    "#=========================================================================================================\n",
    "#============================================================================== K-NN КЛАССИФИКАТОР =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=5, strategy='brute', metric='euclidean', weights=False):\n",
    "        \"\"\"\n",
    "            ===========================================================================================================\n",
    "            param .............. type .... meaning ....................................................................\n",
    "            ===========================================================================================================\n",
    "            k .................. int ..... hyperparameter K ...........................................................\n",
    "            strategy ........... str ..... 'my_own', 'brute', 'kd_tree' or 'ball_tree' - strategy for neighbours search\n",
    "            metric ............. str ..... 'euclidean' or 'cosine' - metric for neighbours search .....................\n",
    "            weights ............ bool .... whether or not use weightened analysis of neighbours .......................\n",
    "            ===========================================================================================================\n",
    "        \"\"\"\n",
    "        if strategy != 'my_own':\n",
    "            self.__knn = NearestNeighbors(n_neighbors=k, algorithm=strategy, metric=metric)\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.strategy = strategy\n",
    "        self.tbs = test_block_size\n",
    "        self.weights = weights\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "            ========================================\n",
    "            Fits the model ..........................\n",
    "        \n",
    "            ========================================\n",
    "            param ..... type ............... meaning\n",
    "            ========================================\n",
    "            X ......... ndarray (float) .... data ..\n",
    "            y ......... ndarray (int) ...... labels\n",
    "            ========================================\n",
    "        \"\"\"\n",
    "        if self.strategy != 'my_own':\n",
    "            self.__knn.fit(X, y)\n",
    "        self.__X = X\n",
    "        self.__y = y\n",
    "    \n",
    "    def find_kneighbors(self, X, return_distance=True):\n",
    "        \"\"\"\n",
    "            =======================================================================================\n",
    "            returns inds of train objs - k neighbors for each obj in X ............................\n",
    "            if return_distance is True, also returns distances for each neighbour for each obj in X\n",
    "        \n",
    "            =======================================================================================\n",
    "            param .............. type ............... meaning .....................................\n",
    "            =======================================================================================\n",
    "            X .................. ndarray (float) .... test data ...................................\n",
    "            return_distance .... bool ............... whether or not return distance from neighbors\n",
    "            =======================================================================================\n",
    "        \"\"\"\n",
    "        if self.strategy != 'my_own':\n",
    "            return self.__knn.kneighbors(X, self.k, return_distance=return_distance)\n",
    "        else:            \n",
    "            res_d = np.zeros((1, self.k))\n",
    "            res_i = np.zeros_like(res_d)\n",
    "            \n",
    "            for i in range(0, X.shape[0], self.tbs):\n",
    "            \n",
    "                if self.metric == 'euclidean':\n",
    "                    dist = euclidean_distance(X[i: i + self.tbs], self.__X)\n",
    "                else:\n",
    "                    dist = cosine_distance(X[i: i + self.tbs], self.__X)\n",
    "                    \n",
    "                bres_i = np.argpartition(dist, kth=np.arange(self.k), axis=1)\n",
    "                bres_i = bres_i[:, :self.k]\n",
    "                \n",
    "                bres_d = np.partition(dist, kth=np.arange(self.k), axis=1)\n",
    "                bres_d = bres_d[:, :self.k]\n",
    "                \n",
    "                res_d = np.concatenate((res_d, bres_d));\n",
    "                res_i = np.concatenate((res_i, bres_i));\n",
    "                \n",
    "            res_d = res_d[1::]\n",
    "            res_i = res_i[1::]\n",
    "                \n",
    "            if return_distance:\n",
    "                return res_d, np.int64(res_i)\n",
    "            else:\n",
    "                return np.int64(res_i)\n",
    "            \n",
    "    def predict(self, X):       \n",
    "        \"\"\"\n",
    "            ===========================================\n",
    "            returns prediction labels for each obj in X\n",
    "        \n",
    "            ===========================================\n",
    "            param .... type ............... meaning ...\n",
    "            ===========================================\n",
    "            X ........ ndarray (float) .... test data .\n",
    "            ===========================================\n",
    "        \"\"\"\n",
    "        yset = set(self.__y)\n",
    "        if self.weights:\n",
    "            dist, ind = self.find_kneighbors(X, True)\n",
    "            w = 1 / (dist + 0.00001)\n",
    "        else:\n",
    "            ind = self.find_kneighbors(X, False)\n",
    "         \n",
    "        kee = self.__y[ind]\n",
    "        if self.weights:\n",
    "                    \n",
    "            raaa = np.zeros((X.shape[0], 1))\n",
    "            klist = []\n",
    "            for k in yset:\n",
    "                aaa = w.copy()\n",
    "                aaa[kee != k] = 0\n",
    "                aaa = np.sum(aaa, axis=1)[:, np.newaxis]\n",
    "                raaa = np.concatenate((raaa, aaa), axis=1)\n",
    "                klist += [k]\n",
    "            raaa = raaa[::, 1::]\n",
    "            daaa = np.argmax(raaa, axis=1)\n",
    "            vaaa = [klist[ik] for ik in daaa]\n",
    "            win = vaaa   \n",
    "                            \n",
    "        else:\n",
    "            win = ([[np.bincount(x).argmax()] for x in kee])\n",
    "    \n",
    "        return np.array(win).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#================================================================================= КРОСС-ВАЛИДАЦИЯ =======\n",
    "#=========================================================================================================\n",
    "    \n",
    "    \n",
    "def kfold(n, n_folds):\n",
    "    \"\"\"\n",
    "        =======================================\n",
    "        indexing for cross-validation folds ...\n",
    "\n",
    "        =======================================\n",
    "        param ...... type .... meaning ........\n",
    "        =======================================\n",
    "        n .......... int ..... num of data objs\n",
    "        n_folds .... int ..... num of folds ...\n",
    "        =======================================\n",
    "    \"\"\"\n",
    "    eif = n // n_folds + 1 # elements in folds + 1\n",
    "    finc = n % n_folds # folds with +1 size\n",
    "    RESList = []\n",
    "    arr = np.arange(n)\n",
    "    \n",
    "    for i in range(finc):\n",
    "        mask = (arr >= eif * i) & (arr < eif * (i + 1))\n",
    "        Test = arr[mask]\n",
    "        Train = arr[~mask]\n",
    "        RESList += [(Train, Test)]\n",
    "        \n",
    "    for i in range(n_folds - finc):\n",
    "        mask = (arr >= eif * finc + (eif - 1) * i) & (arr < eif * finc + (eif - 1) * (i + 1))\n",
    "        Test = arr[mask]\n",
    "        Train = arr[~mask]\n",
    "        RESList += [(Train, Test)]\n",
    "\n",
    "    return RESList\n",
    "\n",
    "def knn_cross_val_score(X, y, k_list, score='accuracy', cv=None, **kwargs):\n",
    "    \"\"\"\n",
    "        ==================================================================\n",
    "        cross-validation .................................................\n",
    "\n",
    "        ==================================================================\n",
    "        param ...... type .... meaning ...................................\n",
    "        ==================================================================\n",
    "        X ......... ndarray (float) .... data ............................\n",
    "        y ......... ndarray (int) ...... true labels .....................\n",
    "        k_list .... list (int) ......... list of hyperparameters k to test\n",
    "        score ..... str ................ Can only be equal to 'accuracy' .\n",
    "        cv ........ list ............... None or another result of kfold .\n",
    "        kwargs .... dict ............... args for KNNCllassifier .........\n",
    "        ==================================================================\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    if cv == None:\n",
    "        cv=kfold(X.shape[0], 3)\n",
    "        \n",
    "    for k in k_list:\n",
    "        res[k] = []  \n",
    "        \n",
    "    knn = KNNClassifier(k=k_list[-1], **kwargs)\n",
    "    yset = set(y)\n",
    "    for itrain, itest in cv:\n",
    "        X_train, X_test = X[itrain], X[itest]\n",
    "        y_train, y_test = y[itrain], y[itest]\n",
    "        knn.fit(X_train, y_train)\n",
    "        dist, ind = knn.find_kneighbors(X_test)\n",
    "        w = 1 / (dist + 1e-5)\n",
    "        \n",
    "        for k in k_list:\n",
    "            kee = y_train[ind[::, :k]]\n",
    "            if knn.weights:\n",
    "                \n",
    "                wo = w[::, :k]\n",
    "                raaa = np.zeros((X_test.shape[0], 1))\n",
    "                klist = []\n",
    "                for ky in yset:\n",
    "                    aaa = wo.copy()\n",
    "                    aaa[kee != ky] = 0\n",
    "                    aaa = np.sum(aaa, axis=1)[:, np.newaxis]\n",
    "                    raaa = np.concatenate((raaa, aaa), axis=1)\n",
    "                    klist += [ky]\n",
    "                raaa = raaa[::, 1::]\n",
    "                daaa = np.argmax(raaa, axis=1)\n",
    "                vaaa = [klist[ik] for ik in daaa]\n",
    "                win = vaaa   \n",
    "                            \n",
    "            else:\n",
    "                win = [np.unique(x)[0] for x in kee]\n",
    "            \n",
    "            if score == 'accuracy':\n",
    "                scores = np.sum(y_test == win) / y_test.shape[0]\n",
    "            res[k] += [scores]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Логистическая регрессия + ГС + СГС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSmoothOracle:\n",
    "    def func(self, w):\n",
    "        return\n",
    "\n",
    "    def grad(self, w):\n",
    "        return\n",
    "\n",
    "    \n",
    "#=========================================================================================================\n",
    "#========================================================================= ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "class BinaryLogistic(BaseSmoothOracle):\n",
    "    def __init__(self, l2_coef):\n",
    "        \"\"\"\n",
    "            =================================================\n",
    "            param ...... type ..... meaning .................\n",
    "            =================================================\n",
    "            l2_coef .... float .... coef of L2 regularization\n",
    "            =================================================\n",
    "        \"\"\"\n",
    "        self.l2_coef = l2_coef\n",
    "     \n",
    "    def func(self, X, y, w):\n",
    "        \"\"\"\n",
    "            =============================================\n",
    "            returns log(1 + exp(-y<w,x>)) for each x in X\n",
    "            (logistic loss function) ....................\n",
    "        \n",
    "            =============================================\n",
    "            param ...... type ................ meaning ..\n",
    "            =============================================\n",
    "            X .... ndarray or csr (float) .... data .....\n",
    "            y .... ndarray (int) ............. labels ...\n",
    "            w .... ndarray (float) ........... weights ..\n",
    "            =============================================\n",
    "        \"\"\"\n",
    "        A = X * w\n",
    "        if type(X) == np.ndarray:\n",
    "            A = np.sum(A, axis=1)\n",
    "        A *= y\n",
    "        A = np.logaddexp(np.zeros_like(A), -A)\n",
    "        A = np.sum(A)\n",
    "        A /= X.shape[0]\n",
    "        A += self.l2_coef / 2 * np.dot(w.T, w)\n",
    "        return A\n",
    "        \n",
    "    def grad(self, X, y, w):\n",
    "        \"\"\"\n",
    "            ===========================================\n",
    "            returns sigmoid(-y<w,x>)*xy for each x in X\n",
    "            (grad of logistic loss function) ..........\n",
    "        \n",
    "            ===========================================\n",
    "            param ...... type ................ meaning \n",
    "            ===========================================\n",
    "            X .... ndarray or csr (float) .... data ...\n",
    "            y .... ndarray (int) ............. labels .\n",
    "            w .... ndarray (float) ........... weights \n",
    "            ===========================================\n",
    "        \"\"\"\n",
    "        if type(X) == np.ndarray:\n",
    "            A = -X * w * y[:, np.newaxis]\n",
    "            A = np.sum(A, axis=1)\n",
    "            A = expit(A)[:, np.newaxis]\n",
    "            B = - X * y[:, np.newaxis]\n",
    "            A = B * A\n",
    "            A = np.sum(A, axis=0)\n",
    "            A /= X.shape[0]\n",
    "            A += self.l2_coef * w\n",
    "        \n",
    "        else:\n",
    "            A = -X * w.T * y\n",
    "            A = expit(A)[:, np.newaxis]\n",
    "            d = ssp.lil_matrix((y.shape[0], y.shape[0]))\n",
    "            d.setdiag(y)\n",
    "            B = -X.T * d\n",
    "            A = B * A\n",
    "            A /= X.shape[0]\n",
    "            A = A.reshape(-1)\n",
    "            A = A + self.l2_coef * w\n",
    "\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#======================================================================= МЕТОД ГРАДИЕНТНОГО СПУСКА =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "class GDClassifier:\n",
    "    def __init__(self, loss_function, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, **kwargs):\n",
    "        \"\"\"\n",
    "            ============================================================================================\n",
    "            GD iteration: parameter := parameter - step_alpha / (step_beta) ^ iter_num * grad(loss_func)\n",
    "            \n",
    "            ============================================================================================\n",
    "            param ............ type ..... meaning ......................................................\n",
    "            ============================================================================================\n",
    "            loss_function .... str ...... loss func for GD .............................................\n",
    "            step_alpha ....... float .... coef of grad shift ...........................................\n",
    "            step_beta ........ float .... coef of grad shift ...........................................\n",
    "            tolerance ........ float .... min difference of near losses to continue.....................\n",
    "            max_iter ......... int ...... max iterations of method .....................................\n",
    "            kwargs ........... dict ..... args for loss class ..........................................\n",
    "            ============================================================================================\n",
    "        \"\"\"\n",
    "        if loss_function == 'binary_logistic':\n",
    "            self.oracle = BinaryLogistic(**kwargs)\n",
    "        self.alpha = step_alpha\n",
    "        self.beta = step_beta\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, Xt=None, yt=None, w_0=None, trace=False, acc=False):\n",
    "        \"\"\"\n",
    "            =================================================================================================\n",
    "            fits the model (GD iterations), returns history of training (time, loss_func, accs for each iter)\n",
    "            \n",
    "            =================================================================================================\n",
    "            param .... type ....................... meaning .................................................\n",
    "            =================================================================================================\n",
    "            X ........ ndarray (float) ............ data ....................................................\n",
    "            y ........ ndarray (int) .............. labels ..................................................\n",
    "            Xt ....... None or ndarray (float) .... test data ...............................................\n",
    "            yt ....... None or ndarray (int) ...... test labels .............................................\n",
    "            w_0 ...... mdarray (float) ............ start weights ...........................................\n",
    "            trace .... bool ....................... whether or not to measure iters times and loss_func .....\n",
    "            acc ...... bool ....................... whether or not to measure iters accuracy ................\n",
    "            =================================================================================================\n",
    "        \"\"\"\n",
    "        self.labels = np.unique(y)\n",
    "        if type(w_0) == np.ndarray:\n",
    "            self.w = w_0\n",
    "        else:\n",
    "            self.w = np.zeros(X.shape[1])\n",
    "        tol_flag = False\n",
    "        iters = 1\n",
    "        loss_1 = self.oracle.func(X, y, self.w)\n",
    "        if trace:\n",
    "            history = {}\n",
    "            history['time'] = [0]\n",
    "            history['func'] = [loss_1]\n",
    "            if acc == True:\n",
    "                history['acc'] = [np.sum(self.predict(Xt) == yt) / len(yt)]\n",
    "        while ((not(tol_flag)) & (iters < self.max_iter + 1)):\n",
    "            \n",
    "            if trace:\n",
    "                start_iter = time.time()\n",
    "            grad = self.oracle.grad(X, y, self.w)\n",
    "            self.w -= self.alpha / np.power(iters, self.beta) * grad\n",
    "            loss_2 = self.oracle.func(X, y, self.w)\n",
    "            if abs(loss_1 - loss_2) < self.tolerance:\n",
    "                tol_flag = True\n",
    "            loss_1 = loss_2\n",
    "            if trace:\n",
    "                end_iter = time.time()\n",
    "                history['time'] += [end_iter - start_iter]\n",
    "                history['func'] += [loss_1]\n",
    "                if acc == True:\n",
    "                    history['acc'] += [np.sum(self.predict(Xt) == yt) / len(yt)]\n",
    "            iters += 1\n",
    "        if trace:\n",
    "            return history\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            =======================================\n",
    "            predicts y for each x in X\n",
    "            \n",
    "            =======================================\n",
    "            param .... type ............... meaning\n",
    "            =======================================\n",
    "            X ........ ndarray (float) .... data ..\n",
    "            =======================================\n",
    "        \"\"\"\n",
    "        A = X * self.w\n",
    "        if type(X) == np.ndarray:\n",
    "            A = np.sum(A, axis=1) \n",
    "        A = np.sign(A)\n",
    "        return A\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "            ===============================================================\n",
    "            predicts propability of (y == +1) and (y == -1) for each x in X\n",
    "            \n",
    "            ===============================================================\n",
    "            param .... type ............... meaning .......................\n",
    "            ===============================================================\n",
    "            X ........ ndarray (float) .... data ..........................\n",
    "            ===============================================================\n",
    "        \"\"\"\n",
    "        A = np.zeros((X.shape[0], self.labels.shape[0]))\n",
    "        B = X * self.w\n",
    "        if type(X) == np.ndarray:\n",
    "            B = np.sum(B, axis=1)\n",
    "        A[:, 0] = expit(B)\n",
    "        A[:, 1] = expit(-B)\n",
    "        return A\n",
    "        \n",
    "        \n",
    "#=========================================================================================================\n",
    "#=============================================================================== СЛУЖЕБНЫЕ ФУНКЦИИ =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "    def get_objective(self, X, y):\n",
    "        return self.oracle.func(X, y, self.w)\n",
    "        \n",
    "    def get_gradient(self, X, y):\n",
    "        return self.oracle.grad(X, y, self.w)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#======================================================= МЕТОД СТОХАСТИЧЕСКОГО ГРАДИЕНТНОГО СПУСКА =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "class SGDClassifier(GDClassifier):\n",
    "    def __init__(self, loss_function, batch_size, step_alpha=1, step_beta=0, \n",
    "                 tolerance=1e-5, max_iter=1000, random_seed=153, **kwargs):\n",
    "        \"\"\"\n",
    "            =======================================================================================================\n",
    "            SGD iteration: parameter := parameter - step_alpha / (step_beta) ^ iter_num * grad(loss_func_for_batch)\n",
    "            predict & predict_proba inherited from GDClassifier ...................................................\n",
    "\n",
    "            =======================================================================================================\n",
    "            param ............ type ..... meaning .................................................................\n",
    "            =======================================================================================================\n",
    "            loss_function .... str ...... loss func for SGD .......................................................\n",
    "            batch_size ....... int ...... size of batch ...........................................................\n",
    "            step_alpha ....... float .... coef of grad shift ......................................................\n",
    "            step_beta ........ float .... coef of grad shift ......................................................\n",
    "            tolerance ........ float .... min difference of near losses to continue................................\n",
    "            max_iter ......... int ...... max iterations of method ................................................\n",
    "            random_seed ...... int ...... seed for data permutation ...............................................\n",
    "            kwargs ........... dict ..... args for loss class .....................................................\n",
    "            =======================================================================================================\n",
    "        \"\"\"\n",
    "        super().__init__(loss_function, step_alpha, step_beta, tolerance, max_iter, **kwargs)\n",
    "        self.rs = random_seed\n",
    "        self.bs = batch_size\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X, y, Xt=None, yt=None, w_0=None, trace=False, acc=False, log_freq=1):\n",
    "        \"\"\"\n",
    "            ==================================================================================================\n",
    "            fits the model (SGD iterations), returns history of training (time, loss_func, accs for each iter)\n",
    "            \n",
    "            ==================================================================================================\n",
    "            param ....... type ....................... meaning ...............................................\n",
    "            ==================================================================================================\n",
    "            X ........... ndarray (float) ............ data ..................................................\n",
    "            y ........... ndarray (int) .............. labels ................................................\n",
    "            Xt .......... None or ndarray (float) .... test data .............................................\n",
    "            yt .......... None or ndarray (int) ...... test labels ...........................................\n",
    "            w_0 ......... mdarray (float) ............ start weights .........................................\n",
    "            trace ....... bool ....................... whether or not to measure iters times and loss_func ...\n",
    "            acc ......... bool ....................... whether or not to measure iters accuracy ..............\n",
    "            log_freq .... float ...................... fraction of using data for one epoch ..................\n",
    "            ==================================================================================================\n",
    "        \"\"\"\n",
    "        np.random.seed(self.rs)\n",
    "        permuts = np.random.permutation(np.arange(len(y)))\n",
    "        \n",
    "        if type(w_0) == np.ndarray:\n",
    "            self.w = w_0\n",
    "        else:\n",
    "            self.w = np.zeros(X.shape[1])\n",
    "        tol_flag = False\n",
    "        iters = 1\n",
    "        loss_1 = self.oracle.func(X, y, self.w)\n",
    "        \n",
    "        if trace:\n",
    "            history = {}\n",
    "            history['epoch_num'] = [0]\n",
    "            history['time'] = [0]\n",
    "            history['func'] = [loss_1]\n",
    "            history['weights_diff'] = [0]\n",
    "            if acc == True:\n",
    "                history['acc'] = [np.sum(self.predict(Xt) == yt) / len(yt)]\n",
    "         \n",
    "        while ((not(tol_flag)) & (iters < self.max_iter + 1)):\n",
    "            permuts = np.random.permutation(np.arange(len(y)))\n",
    "            w_diff = np.zeros_like(self.w)\n",
    "            if trace:\n",
    "                start_iter = time.time()\n",
    "            ind = 0\n",
    "            look_el = 0\n",
    "            while (ind < len(y)) & ((look_el / len(y)) < log_freq):\n",
    "                index_loc = permuts[ind : ind + self.bs]\n",
    "                X_loc = X[index_loc]\n",
    "                y_loc = y[index_loc]\n",
    "                grad = self.oracle.grad(X_loc, y_loc, self.w)\n",
    "                w_diff -= self.alpha / np.power(iters, self.beta) * grad\n",
    "                self.w -= self.alpha / np.power(iters, self.beta) * grad\n",
    "                ind += self.bs\n",
    "                look_el += self.bs\n",
    "            \n",
    "            loss_2 = self.oracle.func(X, y, self.w)\n",
    "            if np.abs(loss_1 - loss_2) < self.tolerance:\n",
    "                tol_flag = True\n",
    "            loss_1 = loss_2\n",
    "            if trace:\n",
    "                end_iter = time.time()\n",
    "                history['epoch_num'] += [iters]\n",
    "                history['time'] += [end_iter - start_iter]\n",
    "                history['func'] += [loss_1]\n",
    "                history['weights_diff'] += [np.dot(w_diff.T, w_diff)]\n",
    "                if acc == True:\n",
    "                    history['acc'] += [np.sum(self.predict(Xt) == yt) / len(yt)]\n",
    "            iters += 1\n",
    "        if trace:\n",
    "            return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Случайный лес + градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#=================================================================================== СЛУЧАЙНЫЙ ЛЕС =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "class RandomForestMSE:\n",
    "    def __init__(self, n_estimators, max_depth=None, feature_subsample_size=None,\n",
    "                 **trees_parameters):\n",
    "        \"\"\"\n",
    "            ===================================================================================\n",
    "            Random Forest (as prediction returns forest mean) .................................\n",
    "\n",
    "            ===================================================================================\n",
    "            param ..................... type ........... meaning ..............................\n",
    "            ===================================================================================\n",
    "            n_estimators .............. int ............ trees number .........................\n",
    "            max_depth ................. int ............ max depth for each tree ..............\n",
    "            feature_subsample_size .... None or int .... volume of using features for each tree\n",
    "            trees_parameters .......... dict ........... args for tree construction  ..........\n",
    "            ===================================================================================\n",
    "        \"\"\"\n",
    "        if feature_subsample_size is None:\n",
    "            self.fs = 'Recommended'\n",
    "        else:\n",
    "            self.fs = 'Choosen'\n",
    "            self.f = feature_subsample_size\n",
    "        self.n = n_estimators\n",
    "        \n",
    "        self.trees = {}\n",
    "        for i in range(self.n):\n",
    "            self.trees[i] = DecisionTreeRegressor(max_depth=max_depth, **trees_parameters)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "            ======================================================\n",
    "            fits the model (each tree)  ..........................\n",
    "            if X_val is not None, saves history of training (RMSE)\n",
    "            \n",
    "            ======================================================\n",
    "            param .... type ....................... meaning ......\n",
    "            ======================================================\n",
    "            X ........ ndarray (float) ............ data .........\n",
    "            y ........ ndarray (float) ............ responses ....\n",
    "            X_val .... None or ndarray (float) .... test data ....\n",
    "            y_val .... None or ndarray (float) .... test responses\n",
    "            ======================================================\n",
    "        \"\"\"\n",
    "        if self.fs == 'Recommended':\n",
    "            self.f = X.shape[1] // 3\n",
    "            \n",
    "        self.featuress = np.zeros((self.f, self.n))\n",
    "        if not X_val is None:\n",
    "            self.RMSE = []\n",
    "        else:\n",
    "            self.RMSE = None\n",
    "            \n",
    "        for i in range(self.n):\n",
    "            train_sample = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "            features = np.random.choice(range(X.shape[1]), self.f, replace=False)\n",
    "            self.featuress[:, i] = features\n",
    "            X_train_local = X[train_sample, :][:, features]\n",
    "            y_train_local = y[train_sample]\n",
    "            self.trees[i].fit(X_train_local, y_train_local)\n",
    "            if not X_val is None:\n",
    "                self.RMSE += [np.sqrt(mean_squared_error(y_val, self.predict(X_val, i + 1)))]\n",
    "        \n",
    "    def predict(self, X, N=None):\n",
    "        \"\"\"\n",
    "            ========================================================\n",
    "            predicts y for each x in X .............................\n",
    "            N regularize volume of trained forest ..................\n",
    "            \n",
    "            ========================================================\n",
    "            param .... type ............... meaning ................\n",
    "            ========================================================\n",
    "            X ........ ndarray (float) .... data ...................\n",
    "            N ........ int or None ........ volume of forest to test\n",
    "            ========================================================\n",
    "        \"\"\"\n",
    "        if N == None or N > self.n:\n",
    "            N = self.n\n",
    "        y_preds = np.zeros((N, X.shape[0]))\n",
    "        for i in range(N):\n",
    "            y_preds[i, :] = self.trees[i].predict(X[:, self.featuress[:, i].astype(int)])\n",
    "        y_pred_res = np.mean(y_preds, axis=0)\n",
    "        return y_pred_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#============================================================================= ГРАДИЕНТНЫЙ БУСТИНГ =======\n",
    "#=========================================================================================================\n",
    "    \n",
    "\n",
    "class GradientBoostingMSE:\n",
    "    def __init__(self, n_estimators, learning_rate=0.1, max_depth=5, feature_subsample_size=None,\n",
    "                 **trees_parameters):\n",
    "        \"\"\"\n",
    "            ====================================================================================\n",
    "            param ...................... type ........... meaning ..............................\n",
    "            ====================================================================================\n",
    "            n_estimators ............... int ............ trees number .........................\n",
    "            learning_rate .............. float .......... learnng rate in models combination ...\n",
    "            max_depth .................. int ............ max depth for each tree ..............\n",
    "            feature_subsample_size ....  None or int .... volume of using features for each tree\n",
    "            trees_parameters ........... dict ........... args for tree construction  ..........\n",
    "            ====================================================================================\n",
    "        \"\"\"\n",
    "        if feature_subsample_size is None:\n",
    "            self.fs = 'Recommended'\n",
    "        else:\n",
    "            self.fs = 'Choosen'\n",
    "            self.f = feature_subsample_size\n",
    "        self.n = n_estimators\n",
    "        self.l = learning_rate\n",
    "        self.algs = {}\n",
    "        for i in range(1, self.n):\n",
    "            self.algs[i] = DecisionTreeRegressor(max_depth=max_depth, **trees_parameters)\n",
    "        \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "            ======================================================\n",
    "            fits the model (each tree)  ..........................\n",
    "            if X_val is not None, saves history of training (RMSE)\n",
    "            \n",
    "            ======================================================\n",
    "            param .... type ....................... meaning ......\n",
    "            ======================================================\n",
    "            X ........ ndarray (float) ............ data .........\n",
    "            y ........ ndarray (float) ............ responses ....\n",
    "            X_val .... None or ndarray (float) .... test data ....\n",
    "            y_val .... None or ndarray (float) .... test responses\n",
    "            ======================================================\n",
    "        \"\"\"\n",
    "        if self.fs == 'Recommended':\n",
    "            self.f = X.shape[1] // 3\n",
    "        \n",
    "        self.featuress = np.zeros((self.f, self.n))\n",
    "        train_sample = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "        y_train_local = y[train_sample]\n",
    "        self.start = np.mean(y_train_local)\n",
    "        self.algs[0] = lambda X: self.start\n",
    "        self.alphas = [1]\n",
    "        if not X_val is None:\n",
    "            self.RMSE = []\n",
    "            self.RMSE += [np.sqrt(mean_squared_error(y_val, self.predict(X_val, 1)))]\n",
    "        else:\n",
    "            self.RMSE = None\n",
    "\n",
    "        for i in range(1, self.n):\n",
    "            train_sample = np.random.randint(0, X.shape[0], X.shape[0])\n",
    "            features = np.random.choice(range(X.shape[1]), self.f, replace=False)\n",
    "            self.featuress[:, i] = features\n",
    "            X_train_local = X[train_sample, :][:, features]\n",
    "            y_train_local = y[train_sample].reshape(-1)\n",
    "            curr_alg_res = self.algs[0](X_train_local)\n",
    "            for j in range(1, i):\n",
    "                curr_alg_res += self.l * self.alphas[j] * self.algs[j].predict(X[train_sample, :][:, self.featuress[:, j].astype(int)])\n",
    "            rs = -2 * (curr_alg_res - y_train_local)\n",
    "            self.algs[i].fit(X_train_local, rs)\n",
    "            \n",
    "            new_alg = lambda alpha: np.sum((curr_alg_res + alpha * self.algs[i].predict(X_train_local) - y_train_local) ** 2)\n",
    "            best_alpha = minimize_scalar(new_alg).x\n",
    "            self.alphas += [best_alpha]\n",
    "            \n",
    "            if not X_val is None:\n",
    "                self.RMSE += [np.sqrt(mean_squared_error(y_val, self.predict(X_val, i + 1)))]\n",
    "\n",
    "    def predict(self, X, N=None):\n",
    "        \"\"\"\n",
    "            ========================================================\n",
    "            predicts y for each x in X .............................\n",
    "            N regularize volume of trained forest ..................\n",
    "            \n",
    "            ========================================================\n",
    "            param .... type ............... meaning ................\n",
    "            ========================================================\n",
    "            X ........ ndarray (float) .... data ...................\n",
    "            N ........ int or None ........ volume of forest to test\n",
    "            ========================================================\n",
    "        \"\"\"\n",
    "        if N == None or N >= self.n:\n",
    "            N = self.n\n",
    "        res = np.ones(X.shape[0]) * self.algs[0](X)\n",
    "        \n",
    "        for i in range(1, N):\n",
    "            a = self.featuress[:, i].astype(int)\n",
    "            res += self.l * self.alphas[i] * self.algs[i].predict(X[:, a])\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Fourier Features + Orthogonal Random Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#========================================================================= RANDOM FOURIER FEATURES =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class RFFPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
    "        \"\"\"\n",
    "            ====================================================================================================\n",
    "            Implements pipeline, which consists of PCA decomposition ...........................................\n",
    "            Random Fourier Features approximation and linear classification model ..............................\n",
    "        \n",
    "            ====================================================================================================\n",
    "            param ......... type .... meaning ..................................................................\n",
    "            ====================================================================================================\n",
    "            n_features .... int ..... amount of synthetic random features generated with RFF approximation .....\n",
    "            new_dim ....... int ..... PCA output size ..........................................................\n",
    "            use_PCA ....... int ..... whether to include PCA preprocessin ......................................\n",
    "            classifier .... bool .... 'svm' or 'logreg', a linear classification model to use on top of pipeline\n",
    "            ====================================================================================================\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.use_PCA = use_PCA\n",
    "        self.new_dim = new_dim\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "            =====================================================================\n",
    "            Fit all parts of algorithm (PCA, RFF, Classification) to training set\n",
    "            returns train time ..................................................\n",
    "            \n",
    "            =====================================================================\n",
    "            param .... type ............... meaning .............................\n",
    "            =====================================================================\n",
    "            X ........ ndarray (float) .... data ................................\n",
    "            y ........ ndarray (int) ...... labels ..............................\n",
    "            =====================================================================\n",
    "        \"\"\"\n",
    "        start_time = time.time() # функция в конце работы возвращает время выполнения\n",
    "        X1 = X.astype(float)\n",
    "        \n",
    "        #====== PCA ================================================================\n",
    "        self.sc1 = StandardScaler()\n",
    "        X1 = self.sc1.fit_transform(X1) # требуется для достижения заданной точности\n",
    "        if self.use_PCA:\n",
    "            self.pca = PCA(n_components=self.new_dim)\n",
    "            X2 = self.pca.fit_transform(X1)\n",
    "        else:\n",
    "            X2 = copy.copy(X1)\n",
    "        #===========================================================================\n",
    "        \n",
    "        \n",
    "        #===== RFF =================================================================\n",
    "        first_obj = np.random.choice(X2.shape[0], int(1e6))\n",
    "        second_obj = np.random.choice(X2.shape[0], int(1e6))\n",
    "        similarity = len(np.where(first_obj == second_obj)[0])\n",
    "        while similarity != 0: # similarity - число совпадающих объектов для оценки квадрата сигмы, этот цикл нужен для обнуления similarity (чтобы избежать смещения оценки)\n",
    "            second_obj[second_obj == first_obj] = np.random.choice(X2.shape[0], similarity)\n",
    "            similarity = len(np.where(first_obj == second_obj)[0])\n",
    "        sigma2 = np.median(np.sum((X2[first_obj] - X2[second_obj]) ** 2, axis=1))\n",
    "        \n",
    "        self.W = np.random.normal(0, 1/sigma2, (X2.shape[1], self.n_features))\n",
    "        self.b = np.random.uniform(-math.pi, math.pi, (self.n_features))\n",
    "        self.sc2 = StandardScaler()\n",
    "        X3 = self.sc2.fit_transform(np.cos(np.matmul(X2, self.W) + self.b))\n",
    "        #===========================================================================\n",
    "        \n",
    "        \n",
    "        #===== Model's fit =========================================================\n",
    "        if self.classifier == 'logreg':\n",
    "            self.model = LogisticRegression(max_iter=200).fit(X3, y)\n",
    "        else:\n",
    "            self.model = LinearSVC(max_iter=250).fit(X3, y)\n",
    "        #===========================================================================\n",
    "        return time.time() - start_time\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "            ==============================================\n",
    "            Apply pipeline to obtain scores for input data\n",
    "            \n",
    "            ==============================================\n",
    "            param .... type ............... meaning ......\n",
    "            ==============================================\n",
    "            X ........ ndarray (float) .... data .........\n",
    "            ==============================================\n",
    "        \"\"\"\n",
    "        X1 = X.astype(float)\n",
    "        X1 = self.sc1.transform(X1)\n",
    "        if self.use_PCA:\n",
    "            X2 = self.pca.transform(X)\n",
    "        else:\n",
    "            X2 = copy.copy(X)\n",
    "        X3 = self.sc1.transform(np.cos(np.matmul(X2, self.W) + self.b))\n",
    "        if self.classifier == 'logreg':\n",
    "            return self.model.predict_proba(X3)\n",
    "        else:\n",
    "            return scipy.expit(self.model.desicion_function(X3))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            ============================================================\n",
    "            Apply pipeline to obtain discrete predictions for input data\n",
    "            \n",
    "            ============================================================\n",
    "            param .... type ............... meaning ....................\n",
    "            ============================================================\n",
    "            X ........ ndarray (float) .... data .......................\n",
    "            ============================================================\n",
    "        \"\"\"\n",
    "        X1 = X.astype(float)\n",
    "        X1 = self.sc1.transform(X1)\n",
    "        if self.use_PCA:\n",
    "            X2 = self.pca.transform(X1)\n",
    "        else:\n",
    "            X2 = copy.copy(X1)  \n",
    "        X3 = self.sc2.transform(np.cos(np.matmul(X2, self.W) + self.b))\n",
    "        return self.model.predict(X3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#====================================================================== ORTHOGONAL RANDOM FEATURES =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ORFPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_features=1000, new_dim=50, use_PCA=True, classifier='logreg'):\n",
    "        \"\"\"\n",
    "            ====================================================================================================\n",
    "            Implements pipeline, which consists of PCA decomposition ...........................................\n",
    "            Orthogonal Random Features approximation and linear classification model ...........................\n",
    "        \n",
    "            ====================================================================================================\n",
    "            param ......... type .... meaning ..................................................................\n",
    "            ====================================================================================================\n",
    "            n_features .... int ..... amount of synthetic random features generated with ORF approximation .....\n",
    "            new_dim ....... int ..... PCA output size ..........................................................\n",
    "            use_PCA ....... int ..... whether to include PCA preprocessin ......................................\n",
    "            classifier .... bool .... 'svm' or 'logreg', a linear classification model to use on top of pipeline\n",
    "            ====================================================================================================\n",
    "        \"\"\"\n",
    "        self.n_features = n_features\n",
    "        self.use_PCA = use_PCA\n",
    "        self.new_dim = new_dim\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "            =====================================================================\n",
    "            Fit all parts of algorithm (PCA, ORF, Classification) to training set\n",
    "            returns train time ..................................................\n",
    "            \n",
    "            =====================================================================\n",
    "            param .... type ............... meaning .............................\n",
    "            =====================================================================\n",
    "            X ........ ndarray (float) .... data ................................\n",
    "            y ........ ndarray (int) ...... labels ..............................\n",
    "            =====================================================================\n",
    "        \"\"\"\n",
    "        start_time = time.time() # функция в конце работы возвращает время выполнения\n",
    "        X1 = X.astype(float)\n",
    "        \n",
    "        #====== PCA ================================================================\n",
    "        self.sc1 = StandardScaler()\n",
    "        X1 = self.sc1.fit_transform(X1) # требуется для достижения заданной точности\n",
    "        if self.use_PCA:\n",
    "            self.pca = PCA(n_components=self.new_dim)\n",
    "            X2 = self.pca.fit_transform(X1)\n",
    "        else:\n",
    "            X2 = copy.copy(X1)\n",
    "        #===========================================================================\n",
    "        \n",
    "        \n",
    "        #===== ORF =================================================================\n",
    "        first_obj = np.random.choice(X2.shape[0], int(1e6))\n",
    "        second_obj = np.random.choice(X2.shape[0], int(1e6))\n",
    "        similarity = len(np.where(first_obj == second_obj)[0])\n",
    "        while similarity != 0: # similarity - число совпадающих объектов для оценки квадрата сигмы, этот цикл нужен для обнуления similarity (чтобы избежать смещения оценки)\n",
    "            second_obj[second_obj == first_obj] = np.random.choice(X2.shape[0], similarity)\n",
    "            similarity = len(np.where(first_obj == second_obj)[0])\n",
    "        sigma2 = np.median(np.sum((X2[first_obj] - X2[second_obj]) ** 2, axis=1))\n",
    "        \n",
    "        d = X2.shape[1]\n",
    "        G = np.random.normal(0, 1/sigma2, (self.n_features, d))\n",
    "        Q, R = np.linalg.qr(G, mode='complete')\n",
    "        Q = Q[:d, :self.n_features]\n",
    "        self.W = Q\n",
    "        self.b = np.random.uniform(-math.pi, math.pi, (self.n_features))\n",
    "        self.sc2 = StandardScaler()\n",
    "        X3 = self.sc2.fit_transform(np.cos(np.matmul(X2, self.W) + self.b))\n",
    "        #===========================================================================\n",
    "        \n",
    "        \n",
    "        #===== Model's fit =========================================================\n",
    "        if self.classifier == 'logreg':\n",
    "            self.model = LogisticRegression(max_iter=200).fit(X3, y)\n",
    "        else:\n",
    "            self.model = LinearSVC(max_iter=50).fit(X3, y)\n",
    "        #===========================================================================\n",
    "        return time.time() - start_time\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "            ==============================================\n",
    "            Apply pipeline to obtain scores for input data\n",
    "            \n",
    "            ==============================================\n",
    "            param .... type ............... meaning ......\n",
    "            ==============================================\n",
    "            X ........ ndarray (float) .... data .........\n",
    "            ==============================================\n",
    "        \"\"\"\n",
    "        X1 = X.astype(float)\n",
    "        X1 = self.sc1.transform(X1)\n",
    "        if self.use_PCA:\n",
    "            X2 = self.pca.transform(X)\n",
    "        else:\n",
    "            X2 = copy.copy(X)\n",
    "        X3 = self.sc1.transform(np.cos(np.matmul(X2, self.W) + self.b))\n",
    "        if self.classifier == 'logreg':\n",
    "            return self.model.predict_proba(X3)\n",
    "        else:\n",
    "            return scipy.expit(self.model.desicion_function(X3))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            ============================================================\n",
    "            Apply pipeline to obtain discrete predictions for input data\n",
    "            \n",
    "            ============================================================\n",
    "            param .... type ............... meaning ....................\n",
    "            ============================================================\n",
    "            X ........ ndarray (float) .... data .......................\n",
    "            ============================================================\n",
    "        \"\"\"\n",
    "        X1 = X.astype(float)\n",
    "        X1 = self.sc1.transform(X1)\n",
    "        if self.use_PCA:\n",
    "            X2 = self.pca.transform(X1)\n",
    "        else:\n",
    "            X2 = copy.copy(X1)  \n",
    "        X3 = self.sc2.transform(np.cos(np.matmul(X2, self.W) + self.b))\n",
    "        return self.model.predict(X3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM-алгоритм в задаче перевода (Word Alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть $S=(s_1,\\ldots,s_n)$ исходное предложение, $T=(t_1,\\ldots,t_m)$ — его перевод. В роли латентных переменных будут выступать выравнивания $A=(a_1,\\ldots,a_m)$ каждого слова в целевом предложении, причём $a_i\\in\\{1,\\ldots,n\\}$ (считаем, что каждое слово в $t$ является переводом какого-то слова из $s$). Параметрами модели является матрица условных вероятностей перевода: каждый её элемент $\\theta(y|x)=p(y|x)$ отражает вероятность того, что переводом слова $x$ с исходного языка на целевой является слово $y$ (нормировка, соответственно, совершается по словарю целевого языка). Правдоподобие латентных переменных и предложения на целевом языке в этой модели записывается так:\n",
    "\n",
    "$$\n",
    "p(A,T|S)=\\prod_{i=1}^m p(a_i)p(t_i|a_i,S)=\\prod_{i=1}^m \\frac{1}{n}\\theta(t_i|s_{a_i}).\n",
    "$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "from xml.dom import minidom\n",
    "import xml\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#=========================================================================================================\n",
    "#================================================================ ВСПОМОГАТЕЛЬНЫЕ СТРУКТУРЫ ДАННЫХ =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SentencePair:\n",
    "    \"\"\"\n",
    "    Contains lists of tokens (strings) for source and target sentence\n",
    "    \"\"\"\n",
    "    source: List[str]\n",
    "    target: List[str]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TokenizedSentencePair:\n",
    "    \"\"\"\n",
    "    Contains arrays of token vocabulary indices (preferably np.int32) for source and target sentence\n",
    "    \"\"\"\n",
    "    source_tokens: np.ndarray\n",
    "    target_tokens: np.ndarray\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class LabeledAlignment:\n",
    "    \"\"\"\n",
    "    Contains arrays of alignments (lists of tuples (source_pos, target_pos)) for a given sentence.\n",
    "    Positions are numbered from 1.\n",
    "    \"\"\"\n",
    "    sure: List[Tuple[int, int]]\n",
    "    possible: List[Tuple[int, int]]\n",
    "        \n",
    "        \n",
    "#=========================================================================================================\n",
    "#====================================================================== ПРЕПРОЦЕССИНГ: ТОКЕНИЗАЦИЯ =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "def extract_sentences(filename: str) -> Tuple[List[SentencePair], List[LabeledAlignment]]:\n",
    "    \"\"\"\n",
    "    Given a file with tokenized parallel sentences and alignments in XML format, return a list of sentence pairs\n",
    "    and alignments for each sentence.\n",
    "\n",
    "    Args:\n",
    "        filename: Name of the file containing XML markup for labeled alignments\n",
    "\n",
    "    Returns:\n",
    "        sentence_pairs: list of `SentencePair`s for each sentence in the file\n",
    "        alignments: list of `LabeledAlignment`s corresponding to these sentences\n",
    "    \"\"\"\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        parsing_file = minidom.parseString(f.read().replace('&', '&amp;'))\n",
    "        sentences = []\n",
    "        tokens = []\n",
    "        lvl1 = parsing_file.getElementsByTagName('sentences')[0]\n",
    "        for lvl2 in lvl1.getElementsByTagName('s'):\n",
    "            sentences += [SentencePair(lvl2.getElementsByTagName('english')[0].childNodes[0].nodeValue.split(' '),\n",
    "                                       lvl2.getElementsByTagName('czech')[0].childNodes[0].nodeValue.split(' '))]\n",
    "            \n",
    "            sures = lvl2.getElementsByTagName('sure')[0].childNodes\n",
    "            if len(sures) > 0:\n",
    "                digits = sures[0].nodeValue.split(' ')\n",
    "                sures = []\n",
    "                for d in digits:\n",
    "                    d0 = d.split('-')\n",
    "                    sures += [(int(d0[0]), int(d0[1]))]\n",
    "            else:\n",
    "                sures = []\n",
    "                \n",
    "            poss = lvl2.getElementsByTagName('possible')[0].childNodes\n",
    "            if len(poss) > 0:\n",
    "                digits = poss[0].nodeValue.split(' ')\n",
    "                poss = []\n",
    "                for d in digits:\n",
    "                    d0 = d.split('-')\n",
    "                    poss += [(int(d0[0]), int(d0[1]))]\n",
    "            else:\n",
    "                poss = []\n",
    "            \n",
    "            tokens += [LabeledAlignment(sures, poss)]\n",
    "        return sentences, tokens\n",
    "\n",
    "def get_token_to_index(sentence_pairs: List[SentencePair], freq_cutoff=None) -> Tuple[Dict[str, int], Dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Given a parallel corpus, create two dictionaries token->index for source and target language.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: list of `SentencePair`s for token frequency estimation\n",
    "        freq_cutoff: if not None, keep only freq_cutoff most frequent tokens in each language\n",
    "\n",
    "    Returns:\n",
    "        source_dict: mapping of token to a unique number (from 0 to vocabulary size) for source language\n",
    "        target_dict: mapping of token to a unique number (from 0 to vocabulary size) target language\n",
    "\n",
    "    \"\"\"\n",
    "    source_dict = {}\n",
    "    target_dict = {}\n",
    "    source_freq = {}\n",
    "    target_freq = {}\n",
    "    \n",
    "    source_reverse = {}\n",
    "    target_reverse = {}\n",
    "    source_ind = 0\n",
    "    target_ind = 0\n",
    "    for sp in sentence_pairs:\n",
    "        eng = sp.source\n",
    "        che = sp.target\n",
    "        for e in eng:\n",
    "            if not (e in source_dict):\n",
    "                source_dict[e] = source_ind\n",
    "                source_reverse[source_ind] = e\n",
    "                source_ind += 1\n",
    "                source_freq[e] = 0\n",
    "            source_freq[e] += 1\n",
    "        for c in che:\n",
    "            if not (c in target_dict):\n",
    "                target_dict[c] = target_ind\n",
    "                target_reverse[target_ind] = c\n",
    "                target_ind += 1\n",
    "                target_freq[c] = 0\n",
    "            target_freq[c] += 1\n",
    "    \n",
    "    if not (freq_cutoff is None):\n",
    "        eng_array = np.array([source_freq[x] for x in source_freq])\n",
    "        che_array = np.array([target_freq[x] for x in target_freq])\n",
    "        if freq_cutoff > len(eng_array) or freq_cutoff > len(che_array):\n",
    "            return source_dict, target_dict\n",
    "        eng_cut = np.argpartition(-eng_array, kth=np.arange(freq_cutoff))[:freq_cutoff]\n",
    "        che_cut = np.argpartition(-che_array, kth=np.arange(freq_cutoff))[:freq_cutoff]\n",
    "        new_source_dict = {source_reverse[x]: y for x, y in zip(eng_cut, np.arange(freq_cutoff))}\n",
    "        new_target_dict = {target_reverse[x]: y for x, y in zip(che_cut, np.arange(freq_cutoff))}\n",
    "        source_dict = new_source_dict\n",
    "        target_dict = new_target_dict\n",
    "            \n",
    "    return source_dict, target_dict\n",
    "\n",
    "\n",
    "def tokenize_sents(sentence_pairs: List[SentencePair], source_dict, target_dict) -> List[TokenizedSentencePair]:\n",
    "    \"\"\"\n",
    "    Given a parallel corpus and token_to_index for each language, transform each pair of sentences from lists\n",
    "    of strings to arrays of integers. If either source or target sentence has no tokens that occur in corresponding\n",
    "    token_to_index, do not include this pair in the result.\n",
    "    \n",
    "    Args:\n",
    "        sentence_pairs: list of `SentencePair`s for transformation\n",
    "        source_dict: mapping of token to a unique number for source language\n",
    "        target_dict: mapping of token to a unique number for target language\n",
    "\n",
    "    Returns:\n",
    "        tokenized_sentence_pairs: sentences from sentence_pairs, tokenized using source_dict and target_dict\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for sp in sentence_pairs:\n",
    "        try:\n",
    "            eng = sp.source\n",
    "            che = sp.target\n",
    "            eng_code = np.zeros(len(eng))\n",
    "            che_code = np.zeros(len(che))\n",
    "            for eng_word, i in zip(eng, np.arange(len(eng))):\n",
    "                if eng_word in source_dict:\n",
    "                    eng_code[i] = source_dict[eng_word]\n",
    "                else:\n",
    "                    raise AssertionError\n",
    "            for che_word, i in zip(che, np.arange(len(che))):\n",
    "                if che_word in target_dict:\n",
    "                    che_code[i] = target_dict[che_word]\n",
    "                else:\n",
    "                    raise AssertionError\n",
    "            res += [TokenizedSentencePair(eng_code.astype(np.int32), che_code.astype(np.int32))]\n",
    "        except:\n",
    "            pass\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#================================================================================ МЕТРИКИ КАЧЕСТВА =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "def compute_precision(reference: List[LabeledAlignment], predicted: List[List[Tuple[int, int]]]) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Computes the numerator and the denominator of the precision for predicted alignments.\n",
    "    Numerator : |predicted and possible|\n",
    "    Denominator: |predicted|\n",
    "    Note that for correct metric values `sure` needs to be a subset of `possible`, but it is not the case for input data.\n",
    "\n",
    "    Args:\n",
    "        reference: list of alignments with fields `possible` and `sure`\n",
    "        predicted: list of alignments, i.e. lists of tuples (source_pos, target_pos)\n",
    "\n",
    "    Returns:\n",
    "        intersection: number of alignments that are both in predicted and possible sets, summed over all sentences\n",
    "        total_predicted: total number of predicted alignments over all sentences\n",
    "    \"\"\"\n",
    "    intersection = 0\n",
    "    total_predicted = 0\n",
    "    for LA, PR in zip(reference, predicted):\n",
    "        total_predicted += len(PR)\n",
    "        intersection += len(list((set(LA.possible) | set(LA.sure)) & set(PR)))\n",
    "    return float(intersection), float(total_predicted)\n",
    "\n",
    "\n",
    "def compute_recall(reference: List[LabeledAlignment], predicted: List[List[Tuple[int, int]]]) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Computes the numerator and the denominator of the recall for predicted alignments.\n",
    "    Numerator : |predicted and sure|\n",
    "    Denominator: |sure|\n",
    "\n",
    "    Args:\n",
    "        reference: list of alignments with fields `possible` and `sure`\n",
    "        predicted: list of alignments, i.e. lists of tuples (source_pos, target_pos)\n",
    "\n",
    "    Returns:\n",
    "        intersection: number of alignments that are both in predicted and sure sets, summed over all sentences\n",
    "        total_predicted: total number of sure alignments over all sentences\n",
    "    \"\"\"\n",
    "    intersection = 0\n",
    "    total_predicted = 0\n",
    "    for LA, PR in zip(reference, predicted):\n",
    "        total_predicted += len(LA.sure)\n",
    "        intersection += len(list(set(LA.sure) & set(PR)))\n",
    "    return float(intersection), float(total_predicted)\n",
    "\n",
    "\n",
    "def compute_aer(reference: List[LabeledAlignment], predicted: List[List[Tuple[int, int]]]) -> float:\n",
    "    \"\"\"\n",
    "    Computes the alignment error rate for predictions.\n",
    "    AER=1-(|predicted and possible|+|predicted and sure|)/(|predicted|+|sure|)\n",
    "    Please use compute_precision and compute_recall to reduce code duplication.\n",
    "\n",
    "    Args:\n",
    "        reference: list of alignments with fields `possible` and `sure`\n",
    "        predicted: list of alignments, i.e. lists of tuples (source_pos, target_pos)\n",
    "\n",
    "    Returns:\n",
    "        aer: the alignment error rate\n",
    "    \"\"\"\n",
    "    precision = compute_precision(reference, predicted)\n",
    "    recall = compute_recall(reference, predicted)\n",
    "    aer = 1 - (precision[0] + recall[0]) / (precision[1] + recall[1])\n",
    "    return aer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================================================================\n",
    "#=============================================================== ПРЕДОПИСАННАЯ МОДЕЛЬ ВЫРАВНИВАНИЯ =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from itertools import product\n",
    "class BaseAligner(ABC):\n",
    "    \"\"\"\n",
    "    Describes a public interface for word alignment models.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self, parallel_corpus: List[TokenizedSentencePair]):\n",
    "        \"\"\"\n",
    "        Estimate alignment model parameters from a collection of parallel sentences.\n",
    "\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def align(self, sentences: List[TokenizedSentencePair]) -> List[List[Tuple[int, int]]]:\n",
    "        \"\"\"\n",
    "        Given a list of tokenized sentences, predict alignments of source and target words.\n",
    "\n",
    "        Args:\n",
    "            sentences: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "\n",
    "        Returns:\n",
    "            alignments: list of alignments for each sentence pair, i.e. lists of tuples (source_pos, target_pos).\n",
    "            Alignment positions in sentences start from 1.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class DiceAligner(BaseAligner):\n",
    "    def __init__(self, num_source_words: int, num_target_words: int, threshold=0.5):\n",
    "        self.cooc = np.zeros((num_source_words, num_target_words), dtype=np.uint32)\n",
    "        self.dice_scores = None\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def fit(self, parallel_corpus):\n",
    "        for sentence in parallel_corpus:\n",
    "            for source_token in np.unique(sentence.source_tokens):\n",
    "                for target_token in np.unique(sentence.target_tokens):\n",
    "                    self.cooc[source_token, target_token] += 1\n",
    "        self.dice_scores = (2 * self.cooc.astype(np.float32) /\n",
    "                            (self.cooc.sum(0, keepdims=True) + self.cooc.sum(1, keepdims=True)))\n",
    "\n",
    "    def align(self, sentences):\n",
    "        result = []\n",
    "        for sentence in sentences:\n",
    "            alignment = []\n",
    "            for (i, source_token), (j, target_token) in product(\n",
    "                    enumerate(sentence.source_tokens, 1),\n",
    "                    enumerate(sentence.target_tokens, 1)):\n",
    "                if self.dice_scores[source_token, target_token] > self.threshold:\n",
    "                    alignment.append((i, j))\n",
    "            result.append(alignment)\n",
    "        return result\n",
    "\n",
    "\n",
    "#=========================================================================================================\n",
    "#================================================================== НАПИСАННАЯ МОДЕЛЬ ВЫРАВНИВАНИЯ =======\n",
    "#=========================================================================================================\n",
    "\n",
    "\n",
    "class WordAligner(BaseAligner):\n",
    "    def __init__(self, num_source_words, num_target_words, num_iters):\n",
    "        self.num_source_words = num_source_words\n",
    "        self.num_target_words = num_target_words\n",
    "        self.translation_probs = np.full((num_source_words, num_target_words), 1 / num_target_words, dtype=np.float32)\n",
    "        self.num_iters = num_iters\n",
    "\n",
    "    def _e_step(self, parallel_corpus: List[TokenizedSentencePair]) -> List[np.array]:\n",
    "        \"\"\"\n",
    "        Given a parallel corpus and current model parameters, get a posterior distribution over alignments for each\n",
    "        sentence pair.\n",
    "\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "\n",
    "        Returns:\n",
    "            posteriors: list of np.arrays with shape (src_len, target_len). posteriors[i][j][k] gives a posterior\n",
    "            probability of target token k to be aligned to source token j in a sentence i.\n",
    "        \"\"\"\n",
    "        posteriors = []\n",
    "        for corp in parallel_corpus:\n",
    "            src = corp.source_tokens\n",
    "            trg = corp.target_tokens\n",
    "            tr_probs = self.translation_probs[np.array(src)[:, np.newaxis], trg]\n",
    "            \n",
    "            nominator = tr_probs\n",
    "            denominator = np.sum(tr_probs, axis=0)\n",
    "            denominator[denominator == 0] = 1\n",
    "            rates = nominator / denominator\n",
    "            post = np.fromfunction(lambda j, k: rates[j, k], \n",
    "                                   shape=(src.shape[0], trg.shape[0]), dtype=np.int8)\n",
    "            posteriors += [post]\n",
    "        return posteriors\n",
    "            \n",
    "\n",
    "    def _compute_elbo(self, parallel_corpus: List[TokenizedSentencePair], posteriors: List[np.array]) -> float:\n",
    "        \"\"\"\n",
    "        Compute evidence (incomplete likelihood) lower bound for a model given data and the posterior distribution\n",
    "        over latent variables.\n",
    "\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "            posteriors: posterior alignment probabilities for parallel sentence pairs (see WordAligner._e_step).\n",
    "\n",
    "        Returns:\n",
    "            elbo: the value of evidence lower bound\n",
    "        \"\"\"\n",
    "        \n",
    "        res = float(0)\n",
    "        for corp, post in zip(parallel_corpus, posteriors):\n",
    "            src = corp.source_tokens\n",
    "            trg = corp.target_tokens\n",
    "            tr_probs = self.translation_probs[src, :][:, trg]\n",
    "            \n",
    "            log_g = np.zeros_like(tr_probs)\n",
    "            log_g[tr_probs > 0] = post[tr_probs > 0] * np.log(tr_probs[tr_probs > 0])\n",
    "            res += float(np.sum(log_g))\n",
    "        return res\n",
    "        \n",
    "\n",
    "    def _m_step(self, parallel_corpus: List[TokenizedSentencePair], posteriors: List[np.array]):\n",
    "        \"\"\"\n",
    "        Update model parameters from a parallel corpus and posterior alignment distribution. Also, compute and return\n",
    "        evidence lower bound after updating the parameters for logging purposes.\n",
    "\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "            posteriors: posterior alignment probabilities for parallel sentence pairs (see WordAligner._e_step).\n",
    "\n",
    "        Returns:\n",
    "            elbo:  the value of evidence lower bound after applying parameter updates\n",
    "        \"\"\"\n",
    "        self.translation_probs[:, :] = 0\n",
    "        for corp, post in zip(parallel_corpus, posteriors):\n",
    "            \n",
    "            src, src_index, src_counts = np.unique(corp.source_tokens, return_counts=True, return_index=True)\n",
    "            trg, trg_index, trg_counts = np.unique(corp.target_tokens, return_counts=True, return_index=True)\n",
    "            blank = post[src_index[:, np.newaxis], trg_index]\n",
    "            blank *= src_counts[:, np.newaxis]\n",
    "            blank *= trg_counts\n",
    "            self.translation_probs[src[:, np.newaxis], trg] += blank\n",
    "        denominator = np.sum(self.translation_probs, axis=1)[:, np.newaxis]\n",
    "        denominator[denominator == 0] = 1.0\n",
    "        self.translation_probs /= denominator\n",
    "        elbo = self._compute_elbo(parallel_corpus, posteriors)\n",
    "        return elbo\n",
    "        \n",
    "\n",
    "    def fit(self, parallel_corpus):\n",
    "        \"\"\"\n",
    "        Same as in the base class, but keep track of ELBO values to make sure that they are non-decreasing.\n",
    "        Sorry for not sticking to my own interface ;)\n",
    "\n",
    "        Args:\n",
    "            parallel_corpus: list of sentences with translations, given as numpy arrays of vocabulary indices\n",
    "\n",
    "        Returns:\n",
    "            history: values of ELBO after each EM-step\n",
    "        \"\"\"\n",
    "        history = []\n",
    "        for i in range(self.num_iters):\n",
    "            posteriors = self._e_step(parallel_corpus)\n",
    "            elbo = self._m_step(parallel_corpus, posteriors)\n",
    "            history.append(elbo)\n",
    "        return history\n",
    "    \n",
    "    def align(self, sentences):\n",
    "        result = []\n",
    "        posts = self._e_step(sentences)\n",
    "        for sentence, post in zip(sentences, posts):\n",
    "            alignment = []\n",
    "            post = np.argmax(post, axis=0)\n",
    "            for i in np.arange(post.shape[0]):\n",
    "                alignment.append((post[i]+1, i+1))\n",
    "            result.append(alignment)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод теории для написанного ЕМ-алгоритма в классе WordAligner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "- # E-шаг\n",
    "\n",
    "Е - шаг заключается в вычислении распределения латентных переменных (в нашем случае - это выравнивания $a_i$) при текущих параметров модели ($\\theta_{ij}$). Мы запишем обновление этого распределения через вероятность $p(A|S,T)$, поразумевая, что связь между оригинальными предложениями $S$ и переводом $T$ регулирует матрица вероятностей $\\theta$.\n",
    "\n",
    "Пересчитаем это распределение латентных переменных сначала для одного предложения (считаем, что $n$ - число слов в оригинальном предложении, $m$ - в переводе). По свойству условных вероятностей:\n",
    "\n",
    "### $$p(A|S,T) = \\frac {p(A,T|S)} {p(T|S)}$$\n",
    "\n",
    "Нужно расписать числитель и знаменатель. Числитель уже расписан в условии задачи:\n",
    "\n",
    "### $$p(A,T|S)=\\prod_{i=1}^m \\frac{1}{n}\\theta(t_i|s_{a_i})$$\n",
    "\n",
    "Знаменатель:\n",
    "\n",
    "#### $ p(T|S) = \\sum_{A=(a_1, a_2, ..., a_m)}p(T,A|S) = \\sum_{a_1=1}^n \\sum_{a_2=1}^n ... \\sum_{a_m=1}^n p(T,A|S) = $ { Опять используем равенство в условии через $\\theta$} = $\\sum_{a_1=1}^n \\sum_{a_2=1}^n ... \\sum_{a_m=1}^n\\prod_{i=1}^m \\frac{1}{n}\\theta(t_i|s_{a_i}) = \\frac{1}{n^m}\\sum_{a_1=1}^n \\sum_{a_2=1}^n ... \\sum_{a_m=1}^n \\prod_{i=1}^m \\theta(t_i|s_{a_i}) = $ {По свойствам дистрибутивности выносим $\\theta(t_i, s_j)$ за знак суммы всех произведений перестановок оставшихся элементов матрицы $\\theta({t_i, s_j}$, и так для каждого элемента, после этого объединим суммы по свойству вероятностей} = $\\frac{1}{n^m}\\prod_{i=1}^m\\sum_{j=1}^n\\theta(t_i, s_j)$, так как $\\sum_{j=1}^n\\theta(t_i, s_j)=1$.\n",
    "\n",
    "Внесём их в одну искомую дробь:\n",
    "\n",
    "### $$p(A|S,T) = \\frac {p(A,T|S)} {p(T|S)} = \\frac {\\frac{1}{n^m}\\prod_{i=1}^m \\theta(t_i|s_{a_i})} {\\frac{1}{n^m}\\prod_{i=1}^m\\sum_{j=1}^n \\theta(t_i|s_{a_j})} = \\prod_{i=1}^m \\frac {\\theta(t_i|s_{a_i})} {\\sum_{j=1}^n \\theta(t_i|s_{a_j})}$$\n",
    "\n",
    "Совершим переход от одного предложения к нескольким. По сути указанный пересчёт нужно произвести для каждого предложения корпуса. В наших обозначениях $R$ - число предложений корпуса, $n_r$, $m_r$ - число слов в оригинальном предложении и переводе, $r=1,2,...,R$. Тогда:\n",
    "\n",
    "### $$p(A_r| S_r, T_r) = \\prod_{i=1}^{m_r}\\frac {\\theta(t_i|s_{a_i})} {\\sum_{j=1}^{n_r} \\theta(t_i|s_{a_j})}$$\n",
    "\n",
    "В этом обновлении распределения латентных переменных и заключён Е-шаг алгоритма. В дальнейшем для отдельного предложения обозначим за $g_{ij}$ - полученную апостериорную вероятность для пары $(t_i, s_j)$, то есть:\n",
    "\n",
    "### $$g_{ij} = \\frac {\\theta(t_i|s_{a_i})} {\\sum_{j=1}^{n} \\theta(t_i|s_{a_j})}$$\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "- # M-шаг\n",
    "\n",
    "\n",
    "M - шаг: максимизация правдоподобия по параметру $\\theta$:\n",
    "\n",
    "### $$Maximize\\ \\ \\ \\int p(A) log \\frac{p(A, S, T| \\theta)}{p(A)}dA\\ \\ \\ by\\ \\ \\theta$$\n",
    "\n",
    "Или, что эквивалентно написанному выше:\n",
    "\n",
    "### $$Maximize\\ \\ \\  \\mathbb{E}_{a_i \\sim p(A)}\\prod_i log(p(a_i, t_i, s_i| \\theta)) \\ \\ \\ by \\ \\ \\theta$$\n",
    "\n",
    "Распишем через параметр вероятностей $\\theta$ (воспользуемя ещё раз свойтвами условной вероятности и представлением через $g_{ij}$, а так же раскроем логарифм произведения в сумму логарифмов):\n",
    "\n",
    "### $$L = \\sum_{i=1}^m\\sum_{j=1}^ng_{ij} log \\theta(t_i, s_j) $$\n",
    "\n",
    "Это выражение - ELBO, оно же и расписанное в данной задаче максимизируемое математическое ожидание (для одного предложения! Для всех предложений при максимизации нужно проссумировать эти функции по корпусу, добавится ещё одна сумма по корпусам, и индексы изменятся для каждого предложения отдельно). Учтём, что макимизация является условной: все столбцы $\\theta$ должны \"суммироваться\" в единицу, так как по условию задачи каждое слово перевода соответствует некоторому слову исходного языка. Значит, событие \"$t_i$ является переводом хотя бы одного слова ихсодного языка\" - достоверно, то есть $\\sum_{i=1}^m\\theta(t_i, s_j) = 1,\\ \\ \\ j = 1, 2, ..., n$ ($n$ условий). \n",
    "\n",
    "Для выполнения M-шага (максимизации) рапишем лагранжиан данной функции:\n",
    "\n",
    "### $$Lagr = \\sum_{i=1}^m\\sum_{j=1}^ng_{ij} log \\theta(t_i, s_j) + \\sum_{j=1}^n \\lambda_{j}(1 - \\sum_{i=1}^m\\theta(t_i, s_j))$$\n",
    "\n",
    "Продифференцируем по двойственным переменным $\\lambda_j$:\n",
    "\n",
    "### $$\\frac{\\partial Lagr}{\\partial \\theta(t_i, s_j)} = \\frac{g_{ij}}{\\theta(t_i, s_j)} - \\lambda_j = 0$$\n",
    "\n",
    "Тогда обновлённые значения параметров матрицы $\\theta$ выражаются через двойственные переменные следующим образом:\n",
    "\n",
    "### $$\\theta(t_i, s_j) = \\frac{g_{ij}}{\\lambda_j}$$\n",
    "\n",
    "Подставим это в лагранжиан:\n",
    "\n",
    "### $$L = \\sum_{i=1}^m\\sum_{j=1}^n g_{ij} log \\frac{g_{ij}}{\\lambda_j} + \\sum_{j=1}^n \\lambda_j(1 - \\sum_{i=1}^m \\frac{g_{ij}}{\\lambda_j}) = \\sum_{i=1}^m\\sum_{j=1}^n g_{ij} (log \\frac{g_{ij}}{\\lambda_j} - 1) + \\sum_{j=1}^n \\lambda_j$$\n",
    "\n",
    "Отдельно отметим, что полная максимизируемая функция является суммой данных L для каждого корпуса. Но для того, чтобы произвести М-шаг, можно разделить их на разные получаемые параметры для корпусов.\n",
    "Эта задача - так же задача оптимизации, но уже безусловная, на переменные $\\lambda_j$. Найдём их, продифференцировав полученный лагранжиан по ним:\n",
    "\n",
    "### $$\\frac{\\partial Lagr}{\\partial \\lambda_j} = -\\frac{1}{\\lambda_j}\\sum_{i=1}^n g_{ij} + 1 = 0$$\n",
    "\n",
    "Выразим $\\lambda_j$:\n",
    "\n",
    "### $$\\lambda_j = \\sum_{i=1}^m g_{ij}$$\n",
    "\n",
    "И, наконец, обновлённые значения для $\\theta$ следующие:\n",
    "\n",
    "### $$\\theta(t_i, s_j) = \\frac{g_{ij}}{\\sum_{i=1}^m g_{ij}}$$\n",
    "\n",
    "В этом обновлении заключается М-шаг ($g_{ij}$ расписано в конце E-шага). Отметим, что элементы матрицы $\\theta$ обновляются для каждого предложения по-отдельности.\n",
    "\n",
    "---\n",
    "\n",
    "- # ELBO\n",
    "\n",
    "Теперь выведем формулу для вычисления самой ELBO. Для этого достаточно взять первую часть расписанного лагранжиана - максимизируемого на М-шаге математического ожидания и подставить в неё найденные обновлённые параметры $\\theta$. Дополнительно к этому для полного подсчёта ELBO нужно аккумулировать сумму по всем корпусам.\n",
    "\n",
    "### $$ELBO = \\sum_{r=1}^R \\sum_{i=1}^{m_r} \\sum_{j=1}^{n_r} g_{ij}^r ( log(\\theta(t_{r(i)}, s_{r(j)}) - log(\\sum_{i=1}^m g_{ij}^r))$$\n",
    "\n",
    "Здесь для того, чтобы \"обработать\" один корпус (одно внешнее слагаемое ELBO) из матрицы $\\theta$ берутся элементы, соответствующие парам слов в соответствующем предложении. $i$, $j$ - порядковые номера этих слов в предложении, а за $r(i)$ и $r(j)$ обозначены отображения этих слов в порядковые номера во всей матрице $\\theta$. Для апостериорных вероятностей $g_{ij}$ добавляется индекс, так как они считаютя на E-шаге для каждого предлложения отдельно. Здесь уже не проводятся выкладки, так как все они были получены ранее, здесь же достаточно расписать способ извлечения элементов матрицы $\\theta$ для алгоритма, что и было сделано."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
